{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handwrite.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyVL4EEGT9l5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6cc033-91d0-421f-9f8b-f80555c019c6"
      },
      "source": [
        "!git clone https://github.com/nobody190/handwriting-synthesis\r\n",
        "!pwd\r\n",
        "%cd handwriting-synthesis\r\n",
        "!pwd\r\n",
        "\r\n",
        "!pip uninstall tensorflow\r\n",
        "!pip install tensorflow==1.6.0\r\n",
        "!pip install svgwrite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'handwriting-synthesis'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 130 (delta 0), reused 0 (delta 0), pack-reused 127\u001b[K\n",
            "Receiving objects: 100% (130/130), 40.21 MiB | 15.45 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n",
            "/content\n",
            "/content/handwriting-synthesis\n",
            "/content/handwriting-synthesis\n",
            "Uninstalling tensorflow-2.4.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.4.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.4.0\n",
            "Collecting tensorflow==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/0f/fbd8bb92459c75db93040f80702ebe4ba83a52cdb6ad930654c31dc0b711/tensorflow-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (45.8MB)\n",
            "\u001b[K     |████████████████████████████████| 45.9MB 102kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.10.0)\n",
            "Collecting tensorboard<1.7.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/67/a8c91665987d359211dcdca5c8b2a7c1e0876eb0702a4383c1e4ff76228d/tensorboard-1.6.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (3.12.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.32.0)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (3.3.3)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.6.0) (51.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (3.7.4.3)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107222 sha256=cfb5a269591202afeac627c26f41402604d2ad2661a68a8ffb4e5279c7691dc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.6.0 tensorflow-1.6.0\n",
            "Collecting svgwrite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/85/1dc25b36c3ac4f3fe285d33065fc0f2ea7bdfb9209d6369e01a3e8ef6252/svgwrite-1.4-py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite\n",
            "Successfully installed svgwrite-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCFkBfYL4VGa",
        "outputId": "fec60e31-8f9e-4219-f152-1f6f4a5d2689"
      },
      "source": [
        "try:\r\n",
        "    from google.colab import drive\r\n",
        "    drive.mount('/content/drive', force_remount=True)\r\n",
        "    COLAB = True\r\n",
        "    print(\"Note: using Google CoLab\")\r\n",
        "except:\r\n",
        "    print(\"Note: not using Google CoLab\")\r\n",
        "    COLAB = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3K9E0UZ4otL"
      },
      "source": [
        "path = '/content/drive/My Drive/handwriting/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLefo2JD4Kmm",
        "outputId": "d35fad8f-8713-4e62-f40a-c9b9423ca3d4"
      },
      "source": [
        "import os\r\n",
        "import logging\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import svgwrite\r\n",
        "import drawing\r\n",
        "import lyrics\r\n",
        "from rnn import rnn\r\n",
        "\r\n",
        "\r\n",
        "class Hand(object):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n",
        "        self.nn = rnn(\r\n",
        "            log_dir='logs',\r\n",
        "            checkpoint_dir='checkpoints',\r\n",
        "            prediction_dir='predictions',\r\n",
        "            learning_rates=[.0001, .00005, .00002],\r\n",
        "            batch_sizes=[32, 64, 64],\r\n",
        "            patiences=[1500, 1000, 500],\r\n",
        "            beta1_decays=[.9, .9, .9],\r\n",
        "            validation_batch_size=32,\r\n",
        "            optimizer='rms',\r\n",
        "            num_training_steps=100000,\r\n",
        "            warm_start_init_step=17900,\r\n",
        "            regularization_constant=0.0,\r\n",
        "            keep_prob=1.0,\r\n",
        "            enable_parameter_averaging=False,\r\n",
        "            min_steps_to_checkpoint=2000,\r\n",
        "            log_interval=20,\r\n",
        "            logging_level=logging.CRITICAL,\r\n",
        "            grad_clip=10,\r\n",
        "            lstm_size=400,\r\n",
        "            output_mixture_components=20,\r\n",
        "            attention_mixture_components=10\r\n",
        "        )\r\n",
        "        self.nn.restore()\r\n",
        "\r\n",
        "    def write(self, filename, lines, biases=None, styles=None, stroke_colors=None, stroke_widths=None):\r\n",
        "        valid_char_set = set(drawing.alphabet)\r\n",
        "        for line_num, line in enumerate(lines):\r\n",
        "            if len(line) > 75:\r\n",
        "                raise ValueError(\r\n",
        "                    (\r\n",
        "                        \"Each line must be at most 75 characters. \"\r\n",
        "                        \"Line {} contains {}\"\r\n",
        "                    ).format(line_num, len(line))\r\n",
        "                )\r\n",
        "\r\n",
        "            for char in line:\r\n",
        "                if char not in valid_char_set:\r\n",
        "                    raise ValueError(\r\n",
        "                        (\r\n",
        "                            \"Invalid character {} detected in line {}. \"\r\n",
        "                            \"Valid character set is {}\"\r\n",
        "                        ).format(char, line_num, valid_char_set)\r\n",
        "                    )\r\n",
        "\r\n",
        "        strokes = self._sample(lines, biases=biases, styles=styles)\r\n",
        "        self._draw(strokes, lines, filename, stroke_colors=stroke_colors, stroke_widths=stroke_widths)\r\n",
        "\r\n",
        "    def _sample(self, lines, biases=None, styles=None):\r\n",
        "        num_samples = len(lines)\r\n",
        "        max_tsteps = 40*max([len(i) for i in lines])\r\n",
        "        biases = biases if biases is not None else [0.5]*num_samples\r\n",
        "\r\n",
        "        x_prime = np.zeros([num_samples, 1200, 3])\r\n",
        "        x_prime_len = np.zeros([num_samples])\r\n",
        "        chars = np.zeros([num_samples, 120])\r\n",
        "        chars_len = np.zeros([num_samples])\r\n",
        "\r\n",
        "        if styles is not None:\r\n",
        "            for i, (cs, style) in enumerate(zip(lines, styles)):\r\n",
        "                x_p = np.load('styles/style-{}-strokes.npy'.format(style))\r\n",
        "                c_p = np.load('styles/style-{}-chars.npy'.format(style)).tostring().decode('utf-8')\r\n",
        "\r\n",
        "                c_p = str(c_p) + \" \" + cs\r\n",
        "                c_p = drawing.encode_ascii(c_p)\r\n",
        "                c_p = np.array(c_p)\r\n",
        "\r\n",
        "                x_prime[i, :len(x_p), :] = x_p\r\n",
        "                x_prime_len[i] = len(x_p)\r\n",
        "                chars[i, :len(c_p)] = c_p\r\n",
        "                chars_len[i] = len(c_p)\r\n",
        "\r\n",
        "        else:\r\n",
        "            for i in range(num_samples):\r\n",
        "                encoded = drawing.encode_ascii(lines[i])\r\n",
        "                chars[i, :len(encoded)] = encoded\r\n",
        "                chars_len[i] = len(encoded)\r\n",
        "\r\n",
        "        [samples] = self.nn.session.run(\r\n",
        "            [self.nn.sampled_sequence],\r\n",
        "            feed_dict={\r\n",
        "                self.nn.prime: styles is not None,\r\n",
        "                self.nn.x_prime: x_prime,\r\n",
        "                self.nn.x_prime_len: x_prime_len,\r\n",
        "                self.nn.num_samples: num_samples,\r\n",
        "                self.nn.sample_tsteps: max_tsteps,\r\n",
        "                self.nn.c: chars,\r\n",
        "                self.nn.c_len: chars_len,\r\n",
        "                self.nn.bias: biases\r\n",
        "            }\r\n",
        "        )\r\n",
        "        samples = [sample[~np.all(sample == 0.0, axis=1)] for sample in samples]\r\n",
        "        return samples\r\n",
        "\r\n",
        "    def _draw(self, strokes, lines, filename, stroke_colors=None, stroke_widths=None):\r\n",
        "        stroke_colors = stroke_colors or ['black']*len(lines)\r\n",
        "        stroke_widths = stroke_widths or [2]*len(lines)\r\n",
        "\r\n",
        "        line_height = 60\r\n",
        "        view_width = 1000\r\n",
        "        view_height = line_height*(len(strokes) + 1)\r\n",
        "\r\n",
        "        dwg = svgwrite.Drawing(filename=filename)\r\n",
        "        dwg.viewbox(width=view_width, height=view_height)\r\n",
        "        dwg.add(dwg.rect(insert=(0, 0), size=(view_width, view_height), fill='white'))\r\n",
        "\r\n",
        "        initial_coord = np.array([0, -(3*line_height / 4)])\r\n",
        "        for offsets, line, color, width in zip(strokes, lines, stroke_colors, stroke_widths):\r\n",
        "\r\n",
        "            if not line:\r\n",
        "                initial_coord[1] -= line_height\r\n",
        "                continue\r\n",
        "\r\n",
        "            offsets[:, :2] *= 1.5\r\n",
        "            strokes = drawing.offsets_to_coords(offsets)\r\n",
        "            strokes = drawing.denoise(strokes)\r\n",
        "            strokes[:, :2] = drawing.align(strokes[:, :2])\r\n",
        "\r\n",
        "            strokes[:, 1] *= -1\r\n",
        "            strokes[:, :2] -= strokes[:, :2].min() + initial_coord\r\n",
        "            strokes[:, 0] += (view_width - strokes[:, 0].max()) / 2\r\n",
        "\r\n",
        "            prev_eos = 1.0\r\n",
        "            p = \"M{},{} \".format(0, 0)\r\n",
        "            for x, y, eos in zip(*strokes.T):\r\n",
        "                p += '{}{},{} '.format('M' if prev_eos == 1.0 else 'L', x, y)\r\n",
        "                prev_eos = eos\r\n",
        "            path = svgwrite.path.Path(p)\r\n",
        "            path = path.stroke(color=color, width=width, linecap='round').fill(\"none\")\r\n",
        "            dwg.add(path)\r\n",
        "\r\n",
        "            initial_coord[1] -= line_height\r\n",
        "\r\n",
        "        dwg.save()\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    hand = Hand()\r\n",
        "\r\n",
        "    with open(path + 'consulta.txt', \"r\") as f:\r\n",
        "        lines = f.read().splitlines() \r\n",
        "\r\n",
        "    #maxlen = len(max(lines, key=len))    \r\n",
        "    #lines = [line + (' ' * (maxlen - len(line))) for line in lines]\r\n",
        "\r\n",
        "    biases = [.75 for i in lines]\r\n",
        "    styles = [9 for i in lines]\r\n",
        "\r\n",
        "    hand.write(\r\n",
        "        filename=path + 'consulta.svg',\r\n",
        "        lines=lines,\r\n",
        "        biases=biases,\r\n",
        "        styles=styles,\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints/model-17900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "Restoring parameters from checkpoints/model-17900\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}